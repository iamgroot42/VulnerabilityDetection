import os
import requests
import time
import sys
import json
from tqdm import tqdm


def readlines(filename):
    #read all lines from a file
    with open(filename, 'r') as f:
        content = f.readlines()
    return(content)


def searchforkeyword(key, commits, access):
  #collect links from the github API response
  maximum = 9999  
  new = 0

  #craft request for Github
  params = (
      ('q', key),('per_page',100)
  )
  myheaders = {'Accept': 'application/vnd.github.cloak-preview', 'Authorization': 'token ' + access}
  nextlink = "https://api.github.com/search/commits"

  iterator = tqdm(range(maximum))
  for _ in iterator:
      limit = 0
      while(limit == 0):
          #request search results
          response = requests.get(nextlink, headers=myheaders,params=params)
          h = response.headers
          if 'X-RateLimit-Remaining' in h:
            limit = int(h['X-RateLimit-Remaining'])
            if limit == 0:
                # Limit of requests per time was reached, sleep to wait until we can request again
                print("Rate limit. Sleep.")
                time.sleep(35)
            #else:
            #  print(h)
      if 'Link' not in h:
        break;
      
      #go through all elements in Github's reply
      content = response.json()
      for k in range(0, len(content["items"])):
          #get relevant info
          repo = content["items"][k]["repository"]["html_url"]
          if repo not in commits:
              #new repository, new commit
              c = {}
              c["url"] = content["items"][k]["url"]
              c["html_url"] = content["items"][k]["html_url"]
              c["message"] = content["items"][k]["commit"]["message"]
              c["sha"] = content["items"][k]["sha"]
              c["keyword"] = key
              commits[repo] = {}
              commits[repo][content["items"][k]["sha"]] = c;
          else:
              if not content["items"][k]["sha"] in commits[repo]:
                #new commit for this already known repository
                new = new + 1
                c = {}
                c["url"] = content["items"][k]["url"]
                c["html_url"] = content["items"][k]["html_url"]
                c["sha"] = content["items"][k]["sha"]
                c["keyword"] = key
                commits[repo][content["items"][k]["sha"]] = c;
                
      #get the links to the next results
      link = h['Link']
      reflinks = analyzelinks(link)
      if "last" in reflinks:
          lastnumber = reflinks["last"].split("&page=")[1]
          maximum = int(lastnumber)-1
      if not "next" in reflinks:
          #done with all that could be collected
          break
      else:
          nextlink = reflinks["next"]
          
  #save the commits that were found
  with open('all_commits.json', 'w') as outfile:
      json.dump(commits, outfile)


def analyzelinks(link):
    #get references to the next page of results
    
    link = link + ","
    reflinks = {}
    while "," in link:
        pos = link.find(",")
        text = link[:pos]
        rest = link[pos+1:]
        try:
          if "\"next\"" in text:
              text = text.split("<")[1]
              text = text.split(">;")[0]
              reflinks["next"]=text
          if "\"prev\"" in text:
              text = text.split("<")[1]
              text = text.split(">;")[0]
              reflinks["prev"]=text
          if "\"first\"" in text:
              text = text.split("<")[1]
              text = text.split(">;")[0]
              reflinks["first"]=text
          if "\"last\"" in text:
              text = text.split("<")[1]
              text = text.split(">;")[0]
              reflinks["last"]=text
        except IndexError as e:
            print(e)
            print("\n")
            print(text)
            print("\n\n")
            sys.exit()
        link = rest
    return(reflinks)


if __name__ == "__main__":
    if not os.path.isfile('access'):
        print("please place a Github access token in this directory.")
        sys.exit()
  
    with open('access', 'r') as accestoken:
        access = accestoken.readline().replace("\n","")

    commits = {}

    #load previously scraped commits
    if os.path.exists('all_commits.json'):
        with open('all_commits.json', 'r') as infile:
            commits = json.load(infile)
    else:
        commits = {}

    keywords = readlines("issue_labels.txt")
    prefixes = readlines("issue_keywords.txt")

    #for all combinations of keywords and prefixes, scrape github for commits
    for k in keywords:
        for pre in prefixes:
                searchforkeyword(k + " " + pre, commits, access);
                exit(0)
