from transformers import AutoTokenizer, AutoModelForMaskedLM, AdamW, DataCollatorForLanguageModeling
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import torch as ch
from tqdm import tqdm


class GithubDatasets(ch.utils.data.Dataset):
    def __init__(self, tokenizer, texts):
        self.encodings = tokenizer(
            texts, truncation=True,
            return_special_tokens_mask=True)

    def __getitem__(self, idx):
        item = {key: ch.tensor(val[idx])
                for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings["input_ids"])


def get_loaders(texts, batch_size, mlm_probability):
    # Split texts into train and validation sets
    print("[Data] Splitting data")
    train_texts, val_texts = train_test_split(
        texts, test_size=0.1, random_state=0)

    # Define tokenizer
    tokenizer = AutoTokenizer.from_pretrained("huggingface/CodeBERTa-small-v1")

    # Wrap in datasets
    print("[Data] Generating tokenized datasets")
    train_ds, val_ds = GithubDatasets(tokenizer, train_texts), GithubDatasets(tokenizer, val_texts)

    # Mask tokens
    print("[Data] Generate mask wrappers")
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm_probability=mlm_probability)
    
    # Make loaders from data
    print("[Data] Creating loaders")
    train_loader = DataLoader(train_ds,
        batch_size=batch_size,
        collate_fn=data_collator,
        shuffle=True)
    val_loader = DataLoader(val_ds,
        batch_size=batch_size,
        collate_fn=data_collator,
        shuffle=False)

    return train_loader, val_loader


def epoch(model, e, loader, optim, is_train=True):
    if is_train:
        model.train()
        optim.zero_grad()
        prefix = "Train"
    else:
        model.eval()
        prefix = "Val"
    
    iterator = tqdm(loader)
    total_loss, total_items = 0, 0
    with ch.set_grad_enabled(is_train):
        for batch in iterator:
            # Shift data to GPU
            for k, v in batch.items():
                batch[k] = v.cuda()

            # Forward pass
            outputs = model(**batch)
            loss = outputs[0]
            loss = loss.mean()

            if is_train:
                # Backprop
                loss.backward()
                optim.step()
                optim.zero_grad()
            
            total_loss += loss.item()
            total_items += len(batch['input_ids'])
        
            iterator.set_description(f"Epoch {e}: {prefix} | Loss: {total_loss / total_items:.4f}")
    
    return total_loss / total_items


def train(texts, model_save_dir, batch_size=16, mlm_probability=0.15, epochs=3, lr=5e-5):
    # Get loaders
    print("[Data] Generating loaders")
    train_loader, val_loader = get_loaders(
        texts, batch_size, mlm_probability=mlm_probability)

    # Build model
    print("[Model] Building model")
    model = AutoModelForMaskedLM.from_pretrained("huggingface/CodeBERTa-small-v1")
    # Shift to multiple GPUS (dataparallel)
    model = ch.nn.DataParallel(model)
    model.cuda()
    model.train()
    optim = AdamW(model.parameters(), lr=lr)
    
    print("[Env] Training")
    for e in range(1, epochs+1):
        train_loss = epoch(model, e, train_loader, optim, is_train=True)
        val_loss = epoch(model, e, val_loader, optim, is_train=False)
        print()
        
        # Save this model inside directory
        ch.save(model.state_dict(), f"{model_save_dir}/model_{e}_tr:{train_loss}_val:{val_loss}.pt")


def read_all_data(path):
    raw_data = []
    with open(path, "r") as f:
        for line in tqdm(f):
            raw_data.append(line)

    data = []
    so_far =  ""
    avg_size = 0
    iterator = tqdm(range(len(raw_data) - 1))
    for i in iterator:
        so_far += raw_data[i]
        if raw_data[i] == "\n" and raw_data[i+1] == "\n":
            if len(so_far.lstrip()) > 0:
                data.append(so_far.lstrip())
                avg_size += data[-1].count("\n")
                so_far = ""
                iterator.set_description("Files: %d | Average size: %1.f" % (len(data), avg_size / len(data)))
        # if len(data) > 1000:
        #     break
    return data


if __name__ == "__main__":
    # Get data
    PYTHON_CODE = """
    def pipeline(
        task: str,
        model: Optional = None,
        framework: Optional[<mask>] = None,
        **kwargs
    ) -> Pipeline:
        pass
    """.lstrip()

    # data = [PYTHON_CODE] * 1000
    data = []
    sources = ["w2v/pythontraining_edit_new.txt", "w2v/pythontraining_edit.txt"]
    for source in sources:
        data += read_all_data(source)

    # Train model
    train(data, model_save_dir="./lm_models", batch_size=10, epochs=10)
