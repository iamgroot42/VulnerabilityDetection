from transformers import AutoTokenizer, AutoModelForMaskedLM, AdamW, DataCollatorForLanguageModeling
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import torch as ch
from tqdm import tqdm


class GithubDatasets(ch.utils.data.Dataset):
    def __init__(self, tokenizer, texts):
        self.encodings = tokenizer(
            texts, truncation=True,
            return_special_tokens_mask=True)

    def __getitem__(self, idx):
        item = {key: ch.tensor(val[idx])
                for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings)


def get_loaders(texts, batch_size, mlm_probability):
    # Split texts into train and validation sets
    train_texts, val_texts = train_test_split(
        texts, test_size=0.1, random_state=0)

    # Define tokenizer
    tokenizer = AutoTokenizer.from_pretrained("huggingface/CodeBERTa-small-v1")

    # Wrap in datasets
    train_ds, val_ds = GithubDatasets(tokenizer, train_texts), GithubDatasets(tokenizer, val_texts)

    # Mask tokens
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm_probability=mlm_probability)
    
    # Make loaders from data
    train_loader = DataLoader(train_ds,
        batch_size=batch_size,
        collate_fn=data_collator,
        shuffle=True)
    val_loader = DataLoader(val_ds,
        batch_size=2 *batch_size,
        collate_fn=data_collator,
        shuffle=False)

    return train_loader, val_loader


def epoch(model, e, loader, optim, is_train=True):
    if is_train:
        model.train()
        optim.zero_grad()
        prefix = "Train"
    else:
        model.eval()
        prefix = "Val"
    
    iterator = tqdm(loader)
    total_loss, total_items = 0, 0
    # with ch.no_grad(not is_train):
    for batch in iterator:
        # Shift data to GPU
        for k, v in batch.items():
            batch[k] = v.cuda()

        # Forward pass
        outputs = model(**batch)
        loss = outputs[0]

        if is_train:
            # Backprop
            loss.backward()
            optim.step()
            
        total_loss += loss.item()
        total_items += len(batch['input_ids'])
        
        iterator.set_description(f"Epoch {e}: {prefix} | Loss: {total_loss / total_items:.4f}")
    
    return total_loss / total_items


def train(texts, model_save_dir, batch_size=16, mlm_probability=0.15, epochs=3, lr=5e-5):
    # Get loaders
    train_loader, val_loader = get_loaders(
        texts, batch_size, mlm_probability=mlm_probability)

    # Build model
    model = AutoModelForMaskedLM.from_pretrained("huggingface/CodeBERTa-small-v1")
    model.train()
    model.cuda()
    optim = AdamW(model.parameters(), lr=lr)
    
    for e in range(1, epochs+1):
        train_loss = epoch(model, e, train_loader, optim, is_train=True)
        val_loss = epoch(model, e, val_loader, optim, is_train=False)
        print()
        
        # Save this model inside directory
        ch.save(model.state_dict(), f"{model_save_dir}/model_{e}_tr:{train_loss}_val:{val_loss}.pt")



if __name__ == "__main__":
    # Get data
    PYTHON_CODE = """
    def pipeline(
        task: str,
        model: Optional = None,
        framework: Optional[<mask>] = None,
        **kwargs
    ) -> Pipeline:
        pass
    """.lstrip()

    # Train model
    train([PYTHON_CODE] * 1000, model_save_dir="./lm_models")
